# Unclassified20250822 — Minto Pyramid Master Table

Coverage: lines processed so far 1–32000 (continuing in 250‑line chunks without pause; table will be updated as new, unique items appear).

| Level | Category | Item | Core Idea (Principle) | When to Use | Pitfalls / Anti‑Patterns | Metrics / Signals | High‑Value Keywords |
|---|---|---|---|---|---|---|---|
| Answer | Foundations | 95% of top‑quality system design achieved by combining cloud well‑architected guardrails and SRE with microservices + EDA, robust data patterns, disciplined delivery/ops, and resilience controls; proactively avoid known anti‑patterns. | Use AWS/Azure Well‑Architected pillars and SRE practices to guide trade‑offs; design for decoupling, observability, and safe change. | Always as baseline governance. | Big Ball of Mud; Golden Hammer; Fallacies of Distributed Computing; Distributed Monolith. | SLOs met; error budget burn healthy; change failure rate low; MTTR short. | AWS/Azure Well‑Architected, SRE, SLOs, Error Budgets, Blameless Postmortems |
| Pillar | Architecture | Microservices | Independently deployable bounded services; clear contracts; autonomy. | Complex domains; team scaling; uneven scaling needs. | Distributed Monolith via tight coupling; shared DB across services. | Lead time; deployment frequency; coupling (change ripple) | Service Boundaries, DDD, API Contracts |
| Pillar | Architecture | Event‑Driven Architecture (EDA) | Asynchronous events for decoupling and resilience. | Reactive workflows; cross‑service propagation; buffering. | Orphan events; unclear ownership; schema drift. | Event throughput; lag; DLQ rate | Async Messaging, Event Schemas |
| Pillar | Architecture | API Gateway + BFF | Single ingress; client‑optimized façades; protocol mediation. | Multi‑client (web/mobile/3P) APIs; anti over/under‑fetch. | Monolith gateway doing business logic. | p95 latency per client; error rate; version skew | API Gateway, Backend‑for‑Frontend |
| Pillar | Architecture | Serverless (targeted) | Elastic, evented units for bursty workloads. | Spiky traffic; infrequent jobs; glue logic. | Cold starts; long‑running tasks; vendor lock‑in. | Cold‑start hit rate; duration; concurrency | Functions, Event Triggers |
| Pattern | Serverless | Cold Start Mitigation | Provisioned concurrency, snapshot/restore (e.g., SnapStart) to remove cold starts. | Latency‑sensitive functions; steady load. | Higher cost; over‑provisioning. | Cold‑start rate; provisioned utilization | Provisioned Concurrency, SnapStart |
| Pillar | Architecture | Modular Monolith | Strong module boundaries inside one deployable; cohesive data model. | Early/mid‑stage products; team not yet scaled. | Hidden coupling; deferred modularity. | Module dependency graph; change ripple | Modular Monolith |
| Pillar | Architecture | Layered Modular Monolith | Enforce layering + module contracts to ease later extraction to services. | Transitional architecture toward microservices. | Leaky layers; god modules. | Dependency rule violations | Layers, Modulith |
| Pillar | Data | Database Sharding | Horizontal data partitioning for throughput/scale. | Very large datasets; hot partitions. | Poor shard key; hotspots; cross‑shard joins. | Per‑shard p99 latency; imbalance | Shard Keys, Rebalancing |
| Pillar | Data | Caching (service/db/edge) | Reduce read latency and backend load. | Read‑heavy workloads; repeated queries. | Stale data; stampede; single‑point cache. | Hit rate; miss penalty; TTL efficacy | TTL, Cache Invalidation |
| Pattern | Caching Policy | Write‑Through | Write to cache and DB synchronously for consistency. | Consistency > write latency. | Higher write latency; write bottlenecks. | Write latency; cache/DB divergence | Write‑Through |
| Pattern | Caching Policy | Write‑Back (Write‑Behind) | Ack on cache; async DB flush (batch). | Write‑heavy; tolerate short‑lived risk. | Data loss on cache failure pre‑flush; complexity. | Flush lag; dirty set size | Write‑Back, Batching |
| Pattern | Caching Policy | Write‑Around | Write DB only; cache on read. | Avoid polluting cache with cold writes. | Immediate reads miss; temporary inconsistency. | Read miss rate for recent writes | Write‑Around |
| Pattern | Caching | In‑Memory Service Cache | Keep hot objects close to compute. | Hot, small working sets. | Eviction storms; memory pressure. | Hit rate; GC/eviction events | Memcached, Redis, DAX |
| Pillar | Data | CQRS | Separate write model from denormalized read models. | Read/write asymmetry; complex queries. | Eventual consistency complexity. | Read staleness; projection lag | CQRS, Projections |
| Pattern | Data | Event Sourcing | Store append‑only events; derive state. | Auditability; temporal queries. | Schema evolution; query complexity. | Event size; replays; projection lag | Event Store, Schemas |
| Pattern | Data Integration | Outbox Pattern | Atomic DB update + event publish via local outbox. | Microservices emitting domain events. | Duplicates; order; table growth. | Relay lag; dedupe rate | CDC, Debezium |
| Pillar | Reliability | Circuit Breaker | Trip on error thresholds; fast‑fail to avoid cascades. | Unstable downstreams; intermittent faults. | Misconfigured thresholds; never resetting. | Open/half‑open ratios; fallback success | Circuit Breaker |
| Pattern | Reliability | Retries with Backoff + Jitter | Space out retries; avoid thundering herd. | Transient network/service errors. | Retry storms; amplified load. | Retry attempt histograms; success after retry | Exponential Backoff, Jitter |
| Pattern | Reliability | Bulkheads | Isolate resource pools to contain failures. | Shared infra under variable load. | Over‑isolation causing under‑utilization. | Saturation per pool; error isolation | Bulkheads |
| Pillar | Delivery/Ops | CI/CD + Progressive Delivery | Safe rollouts: canary, feature flags, A/B; fast rollback. | Frequent deploys; user‑impact risk mgmt. | Config drift; flag debt. | Change failure rate; time to restore | Canary, Feature Flags, A/B |
| Pillar | Delivery/Ops | IaC + GitOps | Declarative infra; ops via PRs; auditability. | Multi‑env parity; repeatability. | Snowflake envs; manual drift. | Drift alerts; rollout sync | Terraform, Argo CD, Flux |
| Pillar | Delivery/Ops | Observability | Metrics, logs, traces across stack. | Complex distributed systems. | Missing golden signals; sampling blind spots. | SLOs, ApDex, RED/USE | Metrics Logs Traces |
| Pillar | Delivery/Ops | SRE Practices | SLOs and error budgets; eliminate toil; blameless culture. | Reliability as product feature. | Ignoring error budget burn; toil accumulation. | Budget burn; incident frequency/MTTR | SLOs, Error Budgets |
| Pillar | Delivery/Ops | Drift Management (IaC) | Detect and reconcile infra drift vs desired state in Git. | Multi‑env fleets; regulated workloads. | Manual hotfixes; untracked changes. | Drift alerts; reconciliation frequency | IaC Drift, GitOps |
| Pillar | Delivery/Ops | Environment Parity | Keep dev/stage/prod behaviorally consistent. | Reduce Heisenbugs; reproducible rollouts. | Hidden config deltas; data shape drift. | Parity checklists; failure parity | Parity |
| Pattern | Reliability | Chaos Engineering | Proactively inject failures to validate resilience and alerting. | Critical paths; before peak events. | Unsafe blast radius; lack of abort. | Experiment success; detection time | Chaos Experiments, Gremlin |
| Reference | Architecture | E‑commerce Checkout Flow | Orchestrated/choreographed saga across payment, inventory, shipping; API gateway; async queues. | Revenue‑critical workflows; high peaks. | Weak compensations; partial failures. | Order completion SLO; queue lag | Saga, SQS/RabbitMQ, API Gateway |
| Migration | Architecture | Strangler Fig | Incrementally replace monolith with services. | Reduce migration risk. | Dual‑write/consistency traps. | Cutover success; legacy surface shrink | Incremental Migration |
| Security | Governance | Well‑Architected Security Pillars | Preventative, detective, responsive, proactive controls; least privilege; zero trust. | All workloads. | DIY crypto; weak secrets mgmt. | Incident MTTD/MTTR; authz failures | Zero Trust, PoLP, KMS, Vault |
| Data Store | Cloud | DynamoDB | Serverless NoSQL; key‑value/document; global tables. | Massive scale with low ops. | Hot partitions; complex queries. | p95/99 latency; RCUs/WCUs | DynamoDB, Global Tables |
| Data Store | Cloud | Spanner | Strong consistency; TrueTime; synchronous replication. | Multi‑region RDBMS needs. | Cost; latency of strong writes. | Commit latency; replica health | Spanner, TrueTime |
| Streams | Platform | Kafka | Durable commit log; stream processing with exactly‑once; event vs processing time; windowing. | Event sourcing; CDC; analytics. | Skewed partitions; consumer lag. | Consumer lag; partition skew | Kafka Streams, EOS |
| Pattern | Consistency | Read‑Your‑Writes | Ensure a client observes its own committed writes. | User sessions; UX correctness. | Cross‑region reads may violate if not pinned. | Violations observed; session pinning rate | Session Consistency |
| Pattern | Consistency | Monotonic Reads/Writes | Prevent observing time‑travel reads; enforce non‑decreasing versions. | Feed/timeline; collaborative edits. | Mixed regions; cache incoherence. | Monotonicity violations; version regressions | Monotonic Consistency |
| Pattern | Consistency | Read Repair & Anti‑Entropy | Heal replica divergence on reads and via background sync. | Eventually consistent stores. | Repair storms; write amplification. | Repair rate; divergence duration | Read Repair, Anti‑Entropy |
| Decision | Process | ADRs + Decision Trees | Document architecturally significant decisions; analyze trade‑offs explicitly. | Major, hard‑to‑reverse choices. | Decisions without context/rationale. | ADR coverage; revisit cadence | ADR, ATAM |
| Anti‑Patterns | Architecture | Distributed Monolith | Tightly coupled “microservices” deploying together. | Anti‑goal; watch for coupling metrics. | Shared databases; cross‑service transactions. | Change ripple; synchronized deploys | Distributed Monolith |
| Anti‑Patterns | Architecture | Big Ball of Mud | No discernible structure; erosion. | Anti‑goal. | Lacking boundaries; ad‑hoc integrations. | Complexity growth; defect clustering | Big Ball of Mud |
| Anti‑Patterns | Practice | Golden Hammer | One tool for all problems. | Anti‑goal. | Ignoring context; cargo cult. | Option analysis documented | Contextual Fit |
| Pattern | Distributed Systems | Leader and Followers | One leader coordinates replication and ordering; followers apply log. | Primary/replica databases; cluster control planes. | Leader hotspots; failover complexity. | Leader election MTTR; replica lag | Leader Election, Replication |
| Pattern | Distributed Systems | Replicated Log | Write‑ahead log replicated across nodes for state sync. | State machine replication; consensus backends. | Log divergence; compaction bugs. | Log lag; compaction metrics | WAL, Raft/Paxos |
| Pattern | Distributed Systems | Majority Quorum | Require majority to commit to avoid split‑brain. | Consensus, replicated state changes. | Availability impact under partitions. | Quorum success rate; commit latency | Quorum, Consensus |
| Pattern | Distributed Systems | Paxos (Consensus) | Two‑phase consensus for safe agreement under faults. | Critical metadata; coordination services. | Complexity; performance tuning. | Proposal/accept latency | Paxos, Consensus |
| Pattern | Distributed Systems | Follower Reads | Serve reads from replicas for scale/latency. | Read‑heavy systems tolerating staleness. | Stale reads; read‑your‑writes violations. | Replica lag; staleness SLO | Read Replicas |
| Pattern | Distributed Systems | Leases | Time‑bound ownership to coordinate safely. | Locking; leader tenure; cache ownership. | Clock skew; expired lease usage. | Lease renewals; skew alarms | Leases, TTL |
| Pattern | Distributed Systems | Logical/Hybrid Clocks | Order events across nodes with Lamport/hybrid clocks. | Versioning; conflict resolution. | Misinterpreting partial order. | Version monotonicity | Lamport, Hybrid Clock |
| Pattern | Distributed Systems | Generation Clock | Monotonically increasing generation number per server to mark epochs and simplify coordination. | Membership changes; epoch tagging. | Mis-synchronization; stale epochs. | Epoch mismatch rate | Generation Clock |
| Pattern | Distributed Systems | Clock‑Bound Wait | Delay operations to cover clock uncertainty and ensure safe ordering. | Cross‑node ordering; TTL/lease safety. | Over‑waiting reduces throughput. | Wait windows; ordering violations | Clock Skew, Safety Window |
| Pattern | Distributed Systems | Emergent Leader | Order nodes by tenure to elect a leader without full election. | Small clusters; minimized coordination. | Unfairness; churn sensitivity. | Leader stability; tenure metrics | Emergent Leader |
| Pattern | Distributed Systems | Consistent Core | Small strongly consistent cluster coordinates larger data cluster. | Coordination planes; metadata ops. | Core hotspots; dependency risk. | Core load; failover success | Consistent Core |
| Pattern | Data Partitioning | Fixed/Key‑Range Partitions | Stable or range‑based shard mapping for scalability. | Range queries; predictable placement. | Hot ranges; manual rebalancing. | Range hotspot metrics | Key‑Range, Fixed Partitions |
| Pattern | Data Partitioning | Consistent Hashing | Distribute keys across nodes uniformly; easy node add/remove. | Cache clusters; sharded data stores. | Hot keys; rehash churn without virtual nodes. | Rebalance volume; key skew | Consistent Hashing |
| Pattern | Performance | Request Batching & Pipelining | Combine or pipeline requests to reduce latency/overhead. | High request volume; chatty protocols. | Head‑of‑line blocking; batching too large. | Batch size; p95 latency | Batching, Pipelining |
| Pattern | Networking | Single‑Socket Channel | Use one TCP connection to preserve in‑order delivery to a server. | When ordering matters per connection. | Connection contention; HOL blocking. | Reorder anomalies; conn utilization | Single Socket |
| Pattern | Concurrency | Singular Update Queue | Single thread processes updates asynchronously to maintain order. | Ordered mutations; simple state machines. | Throughput ceiling; backlog growth. | Queue length; processing delay | Single Thread Queue |
| Pattern | Log Management | Segmented Log & Low‑Water Mark | Split log into segments; track safe truncation point. | Large WALs; streaming systems. | Data loss if truncated early. | Segment count; LWM index | Segmented Log, LWM |
| Pattern | Integration | Change Data Capture (CDC) | Stream DB changes via commit log for near‑real‑time sync. | Event‑driven integration; analytics. | Reordering; schema drifts. | End‑to‑end lag; reorder rate | CDC, Debezium |
| Pattern | Distributed Systems | Request Waiting List | Track deferred client requests and respond when coordination criteria are met. | Multi‑node coordination; quorum responses. | Memory bloat; missed wakeups. | Pending list size; timeout rate | Deferred Responses |
| Pattern | Consistency | Strict 2PL / Serializability | Enforce serializable transactions; lock‑based strictness. | Strong invariants needed. | Contention; deadlocks; throughput loss. | Lock wait time; abort rate | Serializability, 2PL |
| Pattern | Consistency | Quorum Read/Write | R/W quorums to trade latency vs consistency. | Dynamo‑style stores; tunable consistency. | Misconfigured R+W ≤ N; stale reads. | Staleness; quorum failure rate | Quorum Consistency |
| Decision | Trade‑off | CAP Theorem | Under partition: choose Availability vs Consistency; no free lunch. | Distributed system design framing. | Ignoring partition reality; hidden coupling. | Availability vs consistency SLOs during faults | CAP |
| Pattern | Consistency | Vector Clocks | Track causality across replicas to detect conflicts. | Multi‑master replication; offline edits. | High cardinality; complex reconciliation. | Conflict rate; resolution latency | Causality, Vector Clocks |
| Pattern | Consistency | CRDTs | Converge without coordination using mergeable data types. | Collaborative/offline systems; geo‑dist writes. | Limited operations; semantic fit. | Convergence time; merge count | CRDTs |
| Pattern | Replication | Synchronous Replication | Commit only after replicas acknowledge write. | Strong consistency needs; finance; ledgers. | Higher write latency; stall under faults. | Commit latency; replica acks | Sync Replication |
| Platform | Networking | Service Mesh | Sidecars provide mTLS, retries, timeouts, traffic splitting, policy, and telemetry. | Polyglot microservices; standardized comms. | Mesh complexity; double‑retry hazards. | Success after retry; TLS coverage; policy deny rate | Service Mesh, mTLS |
| Pattern | Integration | Ambassador (Sidecar Proxy) | Outbound proxy per service to handle discovery, retries, timeouts, and telemetry consistently. | Heterogeneous clients; cross‑cutting comms concerns. | Config drift; duplicate policies with gateway/mesh. | Retry outcomes; error budget burn | Ambassador, Sidecar |
| Platform | Networking | API Gateway + Mesh Interop | Use gateway for client ingress + mesh for inter‑service comms; avoid double retries. | Large polyglot fleets. | Retry storms; inconsistent policies. | Retry collisions; policy sync health | Gateway–Mesh Interop |
| Pattern | Reliability | Backpressure | Signal upstream to slow producers when consumers saturate. | Message queues; streaming; APIs. | Drop or buffer explosion if mis‑tuned. | Queue depth; rejection rate | Backpressure |
| Pattern | Reliability | Load Shedding & Rate Limiting | Shed excess load and enforce quotas to protect SLOs. | Peak events; abuse protection. | Shedding critical traffic; unfairness. | Shed count; p95 latency under peak | Rate Limiting, Quotas |
| Pillar | Delivery/Ops | Autoscaling | Scale out/in based on demand signals safely. | Variable workloads; cost control. | Flapping; cold starts; laggy signals. | Scale events; utilization; SLO stability | HPA/VPA, Policies |
| Pillar | Delivery/Ops | Blameless Postmortems | Systemic learning from incidents; action items prevent recurrence. | All incident‑driven improvements. | Blame culture; untracked actions. | Action item completion; repeat incident rate | Postmortems, Lessons Learned |
| Pillar | Delivery/Ops | Progressive Delivery | Canary, Blue/Green, and Feature Flags manage risk with real‑user feedback. | Frequent deployments; phased rollouts. | Flag debt; config sprawl. | Canary success rate; rollback rate | Canary, Blue/Green, Feature Flags |
| Pattern | Performance | Queueing Theory (Little’s Law) | L = λW links concurrency, throughput, and latency. | Capacity planning; bottleneck analysis. | Misapplied to non‑stable systems. | W (latency), L (in‑flight), λ (throughput) | Little’s Law |
| Pattern | Integration | Idempotent Receiver | De‑dupe retried messages by unique IDs. | At‑least‑once delivery; retries. | Missing idempotency keys; partial side‑effects. | Duplicate rejection rate | Idempotency |
| Pattern | Data Consistency | Saga | Coordinate cross‑service transactions via choreography or orchestration with compensations. | Multi‑service workflows; no 2PC. | Debug complexity; compensations correctness. | Compensation success; rollback rate | Saga, Orchestration, Choreography |
| Security | Governance | Secrets Management | Centralized vaulting, rotation, least‑privilege access to credentials/keys. | Any production system. | Secrets in code; weak rotation; over‑broad access. | Rotation cadence; secret sprawl | Vault, KMS, Key Vault |
| Security | Supply Chain | SBOM + SLSA Provenance | Track dependencies and build lineage to reduce supply‑chain risk. | Regulated/critical workloads; compliance. | Incomplete SBOM; unverifiable provenance. | SBOM coverage; signed builds | SBOM, SLSA |
| Security | Access Control | Principle of Least Privilege (PoLP) | Grant only minimum necessary permissions; default‑deny; fine‑grained scopes. | All identities and services. | Privilege creep; wildcard grants. | Access reviews; denied‑by‑default rate | Least Privilege, IAM |
| Security | Architecture | Zero Trust Architecture | Never trust by default; verify explicitly; enforce least privilege and assume breach with continuous authz and segmentation. | Internet‑facing and internal services alike. | Implicit trust zones; over‑broad network access. | mTLS coverage; policy enforcement rate; failed authz blocks | Zero Trust, mTLS, Micro‑segmentation |
| Decision | Platform | Golden Paths (Platform Engineering) | Curated templates + paved paths accelerate safe delivery. | Multiple teams; standardize best practices. | Stagnant templates; inflexibility. | Adoption rate; lead‑time reduction | Golden Paths |
| Data Store | NoSQL | Wide‑Column Stores | Column‑family model for massive scale and write throughput. | Time‑series, IoT, large messaging. | Data modeling complexity; eventual consistency. | Write throughput; partition balance | Cassandra, Wide‑Column |
| Anti‑Patterns | Architecture | Fallacies of Distributed Computing | False assumptions about networks cause fragile designs. | Any distributed system. | Assuming reliability, zero latency, homogeneity, unlimited bandwidth. | Post‑incident findings tied to fallacies | Fallacies |
| Decision | Trade‑off | PACELC Theorem | If Partition: choose A vs C; Else: choose L vs C. | Distributed data design decisions. | Implicit choices causing surprises. | Latency–consistency SLO adherence | PACELC |
| Data Store | NoSQL | Graph Databases | Nodes and edges for highly connected data; fast traversal. | Recommendations; fraud; social; knowledge graphs. | Global scans; specialized tooling. | Traversal latency; depth explored | Graph DB, Relationships |
| Pattern | Replication | Leader‑Follower/Multi‑Leader | Replicate for availability and scale; choose topology per consistency/latency needs. | Geo distribution; HA. | Conflicts (multi‑leader); lag. | Replica lag; conflict resolution rate | Replication Strategies |
| Pattern | Data Governance | Soft Delete | Mark rows as deleted to preserve history and referential integrity. | Regulatory/history needs; undo‑friendly deletes. | Zombie records; missed filters. | Soft‑deleted ratio; restore success | Soft Delete |
| Pattern | Data Governance | Entity Auditing | Record created/updated/deleted metadata and changes. | Compliance; debugging; forensics. | Performance overhead; noisy logs. | Audit event volume; gap alerts | Auditing, Change History |
| Pattern | Data Access | N+1 Query Avoidance | Batch/joins to prevent per‑entity queries; prefetch associations. | ORM layers; graph navigation. | Over‑fetching; cache staleness. | Query count per request; p95 latency | N+1, Prefetch, Join Fetch |
| Pattern | Data Caching | Data‑Layer (L2/Query Cache) | ORM second‑level or query cache for repeated reads. | Read‑heavy entities; low write churn. | Stale invalidation; cache poisoning. | Hit rate; invalidation lag | L2 Cache, Query Cache |
| Pattern | Observability | Structured Logging | Key–value logs with consistent schemas; redaction‑ready fields. | Distributed tracing and analytics. | Free‑form logs; PII leaks. | Log schema compliance; correlation rate | Structured Logging |
| Pillar | Delivery/Ops | Externalized Configuration | Manage config outside code; typed sources (env/secret stores); dynamic reloads. | Multi‑env deployments; secrets mgmt. | Drift between envs; runtime misconfig. | Config change success; rollback rate | 12‑Factor Config, Secret Store |
| Security | Supply Chain | Toolchain Pinning | Pin compilers, dependencies, and build images for reproducible builds. | Compliance; stability; provenance. | Stale toolchains; missed patches. | Repro build rate; provenance checks | Reproducible Builds, Pinning |
