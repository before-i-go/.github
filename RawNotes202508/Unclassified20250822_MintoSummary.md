# Unclassified20250822 — Minto Pyramid Master Table

Coverage: lines processed so far 1–3000 (continuing in 250‑line chunks without pause; table will be updated as new, unique items appear).

| Level | Category | Item | Core Idea (Principle) | When to Use | Pitfalls / Anti‑Patterns | Metrics / Signals | High‑Value Keywords |
|---|---|---|---|---|---|---|---|
| Answer | Foundations | 95% of top‑quality system design achieved by combining cloud well‑architected guardrails and SRE with microservices + EDA, robust data patterns, disciplined delivery/ops, and resilience controls; proactively avoid known anti‑patterns. | Use AWS/Azure Well‑Architected pillars and SRE practices to guide trade‑offs; design for decoupling, observability, and safe change. | Always as baseline governance. | Big Ball of Mud; Golden Hammer; Fallacies of Distributed Computing; Distributed Monolith. | SLOs met; error budget burn healthy; change failure rate low; MTTR short. | AWS/Azure Well‑Architected, SRE, SLOs, Error Budgets, Blameless Postmortems |
| Pillar | Architecture | Microservices | Independently deployable bounded services; clear contracts; autonomy. | Complex domains; team scaling; uneven scaling needs. | Distributed Monolith via tight coupling; shared DB across services. | Lead time; deployment frequency; coupling (change ripple) | Service Boundaries, DDD, API Contracts |
| Pillar | Architecture | Event‑Driven Architecture (EDA) | Asynchronous events for decoupling and resilience. | Reactive workflows; cross‑service propagation; buffering. | Orphan events; unclear ownership; schema drift. | Event throughput; lag; DLQ rate | Async Messaging, Event Schemas |
| Pillar | Architecture | API Gateway + BFF | Single ingress; client‑optimized façades; protocol mediation. | Multi‑client (web/mobile/3P) APIs; anti over/under‑fetch. | Monolith gateway doing business logic. | p95 latency per client; error rate; version skew | API Gateway, Backend‑for‑Frontend |
| Pillar | Architecture | Serverless (targeted) | Elastic, evented units for bursty workloads. | Spiky traffic; infrequent jobs; glue logic. | Cold starts; long‑running tasks; vendor lock‑in. | Cold‑start hit rate; duration; concurrency | Functions, Event Triggers |
| Pillar | Data | Database Sharding | Horizontal data partitioning for throughput/scale. | Very large datasets; hot partitions. | Poor shard key; hotspots; cross‑shard joins. | Per‑shard p99 latency; imbalance | Shard Keys, Rebalancing |
| Pillar | Data | Caching (service/db/edge) | Reduce read latency and backend load. | Read‑heavy workloads; repeated queries. | Stale data; stampede; single‑point cache. | Hit rate; miss penalty; TTL efficacy | TTL, Cache Invalidation |
| Pattern | Caching Policy | Write‑Through | Write to cache and DB synchronously for consistency. | Consistency > write latency. | Higher write latency; write bottlenecks. | Write latency; cache/DB divergence | Write‑Through |
| Pattern | Caching Policy | Write‑Back (Write‑Behind) | Ack on cache; async DB flush (batch). | Write‑heavy; tolerate short‑lived risk. | Data loss on cache failure pre‑flush; complexity. | Flush lag; dirty set size | Write‑Back, Batching |
| Pattern | Caching Policy | Write‑Around | Write DB only; cache on read. | Avoid polluting cache with cold writes. | Immediate reads miss; temporary inconsistency. | Read miss rate for recent writes | Write‑Around |
| Pattern | Caching | In‑Memory Service Cache | Keep hot objects close to compute. | Hot, small working sets. | Eviction storms; memory pressure. | Hit rate; GC/eviction events | Memcached, Redis, DAX |
| Pillar | Data | CQRS | Separate write model from denormalized read models. | Read/write asymmetry; complex queries. | Eventual consistency complexity. | Read staleness; projection lag | CQRS, Projections |
| Pattern | Data | Event Sourcing | Store append‑only events; derive state. | Auditability; temporal queries. | Schema evolution; query complexity. | Event size; replays; projection lag | Event Store, Schemas |
| Pattern | Data Integration | Outbox Pattern | Atomic DB update + event publish via local outbox. | Microservices emitting domain events. | Duplicates; order; table growth. | Relay lag; dedupe rate | CDC, Debezium |
| Pillar | Reliability | Circuit Breaker | Trip on error thresholds; fast‑fail to avoid cascades. | Unstable downstreams; intermittent faults. | Misconfigured thresholds; never resetting. | Open/half‑open ratios; fallback success | Circuit Breaker |
| Pattern | Reliability | Retries with Backoff + Jitter | Space out retries; avoid thundering herd. | Transient network/service errors. | Retry storms; amplified load. | Retry attempt histograms; success after retry | Exponential Backoff, Jitter |
| Pattern | Reliability | Bulkheads | Isolate resource pools to contain failures. | Shared infra under variable load. | Over‑isolation causing under‑utilization. | Saturation per pool; error isolation | Bulkheads |
| Pillar | Delivery/Ops | CI/CD + Progressive Delivery | Safe rollouts: canary, feature flags, A/B; fast rollback. | Frequent deploys; user‑impact risk mgmt. | Config drift; flag debt. | Change failure rate; time to restore | Canary, Feature Flags, A/B |
| Pillar | Delivery/Ops | IaC + GitOps | Declarative infra; ops via PRs; auditability. | Multi‑env parity; repeatability. | Snowflake envs; manual drift. | Drift alerts; rollout sync | Terraform, Argo CD, Flux |
| Pillar | Delivery/Ops | Observability | Metrics, logs, traces across stack. | Complex distributed systems. | Missing golden signals; sampling blind spots. | SLOs, ApDex, RED/USE | Metrics Logs Traces |
| Pillar | Delivery/Ops | SRE Practices | SLOs and error budgets; eliminate toil; blameless culture. | Reliability as product feature. | Ignoring error budget burn; toil accumulation. | Budget burn; incident frequency/MTTR | SLOs, Error Budgets |
| Pattern | Reliability | Chaos Engineering | Proactively inject failures to validate resilience and alerting. | Critical paths; before peak events. | Unsafe blast radius; lack of abort. | Experiment success; detection time | Chaos Experiments, Gremlin |
| Reference | Architecture | E‑commerce Checkout Flow | Orchestrated/choreographed saga across payment, inventory, shipping; API gateway; async queues. | Revenue‑critical workflows; high peaks. | Weak compensations; partial failures. | Order completion SLO; queue lag | Saga, SQS/RabbitMQ, API Gateway |
| Migration | Architecture | Strangler Fig | Incrementally replace monolith with services. | Reduce migration risk. | Dual‑write/consistency traps. | Cutover success; legacy surface shrink | Incremental Migration |
| Security | Governance | Well‑Architected Security Pillars | Preventative, detective, responsive, proactive controls; least privilege; zero trust. | All workloads. | DIY crypto; weak secrets mgmt. | Incident MTTD/MTTR; authz failures | Zero Trust, PoLP, KMS, Vault |
| Data Store | Cloud | DynamoDB | Serverless NoSQL; key‑value/document; global tables. | Massive scale with low ops. | Hot partitions; complex queries. | p95/99 latency; RCUs/WCUs | DynamoDB, Global Tables |
| Data Store | Cloud | Spanner | Strong consistency; TrueTime; synchronous replication. | Multi‑region RDBMS needs. | Cost; latency of strong writes. | Commit latency; replica health | Spanner, TrueTime |
| Streams | Platform | Kafka | Durable commit log; stream processing with exactly‑once; event vs processing time; windowing. | Event sourcing; CDC; analytics. | Skewed partitions; consumer lag. | Consumer lag; partition skew | Kafka Streams, EOS |
| Pattern | Consistency | Read Repair & Anti‑Entropy | Heal replica divergence on reads and via background sync. | Eventually consistent stores. | Repair storms; write amplification. | Repair rate; divergence duration | Read Repair, Anti‑Entropy |
| Decision | Process | ADRs + Decision Trees | Document architecturally significant decisions; analyze trade‑offs explicitly. | Major, hard‑to‑reverse choices. | Decisions without context/rationale. | ADR coverage; revisit cadence | ADR, ATAM |
| Anti‑Patterns | Architecture | Distributed Monolith | Tightly coupled “microservices” deploying together. | Anti‑goal; watch for coupling metrics. | Shared databases; cross‑service transactions. | Change ripple; synchronized deploys | Distributed Monolith |
| Anti‑Patterns | Architecture | Big Ball of Mud | No discernible structure; erosion. | Anti‑goal. | Lacking boundaries; ad‑hoc integrations. | Complexity growth; defect clustering | Big Ball of Mud |
| Anti‑Patterns | Practice | Golden Hammer | One tool for all problems. | Anti‑goal. | Ignoring context; cargo cult. | Option analysis documented | Contextual Fit |
| Pattern | Distributed Systems | Leader and Followers | One leader coordinates replication and ordering; followers apply log. | Primary/replica databases; cluster control planes. | Leader hotspots; failover complexity. | Leader election MTTR; replica lag | Leader Election, Replication |
| Pattern | Distributed Systems | Replicated Log | Write‑ahead log replicated across nodes for state sync. | State machine replication; consensus backends. | Log divergence; compaction bugs. | Log lag; compaction metrics | WAL, Raft/Paxos |
| Pattern | Distributed Systems | Majority Quorum | Require majority to commit to avoid split‑brain. | Consensus, replicated state changes. | Availability impact under partitions. | Quorum success rate; commit latency | Quorum, Consensus |
| Pattern | Distributed Systems | Paxos (Consensus) | Two‑phase consensus for safe agreement under faults. | Critical metadata; coordination services. | Complexity; performance tuning. | Proposal/accept latency | Paxos, Consensus |
| Pattern | Distributed Systems | Follower Reads | Serve reads from replicas for scale/latency. | Read‑heavy systems tolerating staleness. | Stale reads; read‑your‑writes violations. | Replica lag; staleness SLO | Read Replicas |
| Pattern | Distributed Systems | Leases | Time‑bound ownership to coordinate safely. | Locking; leader tenure; cache ownership. | Clock skew; expired lease usage. | Lease renewals; skew alarms | Leases, TTL |
| Pattern | Distributed Systems | Logical/Hybrid Clocks | Order events across nodes with Lamport/hybrid clocks. | Versioning; conflict resolution. | Misinterpreting partial order. | Version monotonicity | Lamport, Hybrid Clock |
| Pattern | Data Partitioning | Fixed/Key‑Range Partitions | Stable or range‑based shard mapping for scalability. | Range queries; predictable placement. | Hot ranges; manual rebalancing. | Range hotspot metrics | Key‑Range, Fixed Partitions |
| Pattern | Performance | Request Batching & Pipelining | Combine or pipeline requests to reduce latency/overhead. | High request volume; chatty protocols. | Head‑of‑line blocking; batching too large. | Batch size; p95 latency | Batching, Pipelining |
| Pattern | Log Management | Segmented Log & Low‑Water Mark | Split log into segments; track safe truncation point. | Large WALs; streaming systems. | Data loss if truncated early. | Segment count; LWM index | Segmented Log, LWM |
| Pattern | Integration | Change Data Capture (CDC) | Stream DB changes via commit log for near‑real‑time sync. | Event‑driven integration; analytics. | Reordering; schema drifts. | End‑to‑end lag; reorder rate | CDC, Debezium |
| Pattern | Consistency | Strict 2PL / Serializability | Enforce serializable transactions; lock‑based strictness. | Strong invariants needed. | Contention; deadlocks; throughput loss. | Lock wait time; abort rate | Serializability, 2PL |
| Pattern | Consistency | Quorum Read/Write | R/W quorums to trade latency vs consistency. | Dynamo‑style stores; tunable consistency. | Misconfigured R+W ≤ N; stale reads. | Staleness; quorum failure rate | Quorum Consistency |
| Decision | Trade‑off | PACELC Theorem | If Partition: choose A vs C; Else: choose L vs C. | Distributed data design decisions. | Implicit choices causing surprises. | Latency–consistency SLO adherence | PACELC |
| Data Store | NoSQL | Graph Databases | Nodes and edges for highly connected data; fast traversal. | Recommendations; fraud; social; knowledge graphs. | Global scans; specialized tooling. | Traversal latency; depth explored | Graph DB, Relationships |
| Pattern | Replication | Leader‑Follower/Multi‑Leader | Replicate for availability and scale; choose topology per consistency/latency needs. | Geo distribution; HA. | Conflicts (multi‑leader); lag. | Replica lag; conflict resolution rate | Replication Strategies |
